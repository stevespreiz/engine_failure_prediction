{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data_dir = \"../data/processed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading \n",
    "train_file = np.loadtxt(data_dir+\"windowed_train_FD001.txt\",delimiter=\",\")\n",
    "test_file = np.loadtxt(data_dir +\"standardized_test_FD001.txt\",delimiter=\",\")\n",
    "\n",
    "train_x = train_file[:,2:]\n",
    "train_y = train_file[:,1]\n",
    "\n",
    "test_units = test_file[:,0]\n",
    "test_x = test_file[:,2:]\n",
    "test_y = test_file[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_tw = 50\n",
    "num_samples = int(train_file.shape[0]/N_tw)\n",
    "num_features = 24\n",
    "\n",
    "data_x = torch.zeros(num_samples,N_tw,num_features)\n",
    "data_y = torch.zeros(num_samples,N_tw,1)\n",
    "\n",
    "for i in range(num_samples):\n",
    "    for t in range(N_tw):\n",
    "        data_y[i][t][0] = torch.tensor(train_y[i*N_tw+t])\n",
    "        data_x[i][t][:] = torch.tensor(train_x[i*N_tw+t,:])\n",
    "        # for j in range(num_features):\n",
    "        #     data_x[i][t][0][j] = train_x[i*N_tw+t,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EngineDataset(Dataset):\n",
    "    def __init__(self,data_x,data_y):\n",
    "        self.x = data_x\n",
    "        self.y = data_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.size()[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15731\n",
      "torch.Size([15731, 50, 24])\n",
      "torch.Size([128, 50, 24])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "train_data = EngineDataset(data_x, data_y)\n",
    "print(train_data.__len__())\n",
    "print(train_data.x.size())\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "print(next(iter(train_loader))[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "#set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Neural Net\n",
    "class GRUNet(nn.Module):\n",
    "    def __init__(self,input_dim,hidden_dim,output_dim,n_layers,drop_prob=0.5):\n",
    "        super(GRUNet,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #batch_first = True : input dims are (batch_size,seq_length,n_features)\n",
    "        self.gru = nn.GRU(input_dim,hidden_dim,n_layers,batch_first=True,dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim,output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x,h):\n",
    "        out,h = self.gru(x,h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "\n",
    "        return out,h\n",
    "\n",
    "    def init_hidden(self,batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers,batch_size,self.hidden_dim).zero_().to(device)\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, learn_rate, hidden_dim=256, EPOCHS=5):\n",
    "    # Setting common hyperparameters\n",
    "    input_dim = next(iter(train_loader))[0].size()[2]\n",
    "    output_dim = 1\n",
    "    n_layers = 2\n",
    "    # Instantiating the models\n",
    "    model = GRUNet(input_dim, hidden_dim, output_dim, n_layers)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    batch_size = train_loader.batch_size\n",
    "    \n",
    "    # Defining loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
    "    \n",
    "    model.train()\n",
    "    print(\"Starting Training of GRU model\")\n",
    "    epoch_times = []\n",
    "    # Start training loop\n",
    "    for epoch in range(1,EPOCHS+1):\n",
    "        start_time = time.process_time()\n",
    "        h = model.init_hidden(batch_size)\n",
    "        avg_loss = 0.\n",
    "        counter = 0\n",
    "        for x, label in train_loader:\n",
    "            counter += 1\n",
    "            h = h.data\n",
    "\n",
    "            model.zero_grad()\n",
    "            label = label[:,-1]\n",
    "            \n",
    "            out, h = model(x.to(device).float(), h)\n",
    "\n",
    "            loss = criterion(out, label.to(device).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()\n",
    "            if counter%200 == 0:\n",
    "                print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, counter, len(train_loader), avg_loss/counter))\n",
    "        current_time = time.process_time()        \n",
    "        print(\"Epoch {}/{} Done, Total Loss: {}\".format(epoch, EPOCHS, avg_loss/len(train_loader)))\n",
    "        print(\"Total Time Elapsed: {} seconds\".format(str(current_time-start_time)))\n",
    "        epoch_times.append(current_time-start_time)\n",
    "    print(\"Total Training Time: {} seconds\".format(str(sum(epoch_times))))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training of GRU model\n",
      "Epoch 1/50 Done, Total Loss: 0.11341557288389714\n",
      "Total Time Elapsed: 0.859375 seconds\n",
      "Epoch 2/50 Done, Total Loss: 0.04943844676017761\n",
      "Total Time Elapsed: 0.6875 seconds\n",
      "Epoch 3/50 Done, Total Loss: 0.04173457969102214\n",
      "Total Time Elapsed: 0.6875 seconds\n",
      "Epoch 4/50 Done, Total Loss: 0.0394445481084165\n",
      "Total Time Elapsed: 0.71875 seconds\n",
      "Epoch 5/50 Done, Total Loss: 0.03924207396988497\n",
      "Total Time Elapsed: 0.671875 seconds\n",
      "Epoch 6/50 Done, Total Loss: 0.03590816042584474\n",
      "Total Time Elapsed: 0.671875 seconds\n",
      "Epoch 7/50 Done, Total Loss: 0.034963297428654845\n",
      "Total Time Elapsed: 0.65625 seconds\n",
      "Epoch 8/50 Done, Total Loss: 0.034588453207226075\n",
      "Total Time Elapsed: 0.671875 seconds\n",
      "Epoch 9/50 Done, Total Loss: 0.032883298690201804\n",
      "Total Time Elapsed: 0.703125 seconds\n",
      "Epoch 10/50 Done, Total Loss: 0.03158769362651911\n",
      "Total Time Elapsed: 0.6875 seconds\n",
      "Epoch 11/50 Done, Total Loss: 0.02972192892835277\n",
      "Total Time Elapsed: 0.671875 seconds\n",
      "Epoch 12/50 Done, Total Loss: 0.02835638689823815\n",
      "Total Time Elapsed: 0.6875 seconds\n",
      "Epoch 13/50 Done, Total Loss: 0.025298188700050604\n",
      "Total Time Elapsed: 0.671875 seconds\n",
      "Epoch 14/50 Done, Total Loss: 0.02160195092533211\n",
      "Total Time Elapsed: 0.671875 seconds\n",
      "Epoch 15/50 Done, Total Loss: 0.017344003077596426\n",
      "Total Time Elapsed: 0.65625 seconds\n",
      "Epoch 16/50 Done, Total Loss: 0.01361250059037912\n",
      "Total Time Elapsed: 0.671875 seconds\n",
      "Epoch 17/50 Done, Total Loss: 0.010894840384726642\n",
      "Total Time Elapsed: 0.671875 seconds\n",
      "Epoch 18/50 Done, Total Loss: 0.008752732320123763\n",
      "Total Time Elapsed: 0.671875 seconds\n",
      "Epoch 19/50 Done, Total Loss: 0.007155456213799656\n",
      "Total Time Elapsed: 0.640625 seconds\n",
      "Epoch 20/50 Done, Total Loss: 0.0055587420728607255\n",
      "Total Time Elapsed: 0.671875 seconds\n",
      "Epoch 21/50 Done, Total Loss: 0.0050666235715578324\n",
      "Total Time Elapsed: 0.671875 seconds\n",
      "Epoch 22/50 Done, Total Loss: 0.004043025050510759\n",
      "Total Time Elapsed: 0.65625 seconds\n",
      "Epoch 23/50 Done, Total Loss: 0.003475953981310862\n",
      "Total Time Elapsed: 0.671875 seconds\n",
      "Epoch 24/50 Done, Total Loss: 0.0032472860881081612\n",
      "Total Time Elapsed: 0.671875 seconds\n",
      "Epoch 25/50 Done, Total Loss: 0.002497798886310431\n",
      "Total Time Elapsed: 0.640625 seconds\n",
      "Epoch 26/50 Done, Total Loss: 0.002263989301444199\n",
      "Total Time Elapsed: 0.65625 seconds\n",
      "Epoch 27/50 Done, Total Loss: 0.0019696407987675095\n",
      "Total Time Elapsed: 0.671875 seconds\n",
      "Epoch 28/50 Done, Total Loss: 0.001925812439695306\n",
      "Total Time Elapsed: 0.671875 seconds\n",
      "Epoch 29/50 Done, Total Loss: 0.0016468765364852963\n",
      "Total Time Elapsed: 0.671875 seconds\n",
      "Epoch 30/50 Done, Total Loss: 0.001560219234336534\n",
      "Total Time Elapsed: 0.671875 seconds\n",
      "Epoch 31/50 Done, Total Loss: 0.0015405049665876832\n",
      "Total Time Elapsed: 0.640625 seconds\n",
      "Epoch 32/50 Done, Total Loss: 0.0014458175526252475\n",
      "Total Time Elapsed: 0.671875 seconds\n",
      "Epoch 33/50 Done, Total Loss: 0.0014940944755999525\n",
      "Total Time Elapsed: 0.65625 seconds\n",
      "Epoch 34/50 Done, Total Loss: 0.0012132785498683693\n",
      "Total Time Elapsed: 0.65625 seconds\n",
      "Epoch 35/50 Done, Total Loss: 0.001211303811077578\n",
      "Total Time Elapsed: 0.671875 seconds\n",
      "Epoch 36/50 Done, Total Loss: 0.0011908995062990693\n",
      "Total Time Elapsed: 0.671875 seconds\n",
      "Epoch 37/50 Done, Total Loss: 0.0010530747965619457\n",
      "Total Time Elapsed: 0.65625 seconds\n",
      "Epoch 38/50 Done, Total Loss: 0.0009506572511230335\n",
      "Total Time Elapsed: 0.671875 seconds\n",
      "Epoch 39/50 Done, Total Loss: 0.0008678207208295582\n",
      "Total Time Elapsed: 0.65625 seconds\n",
      "Epoch 40/50 Done, Total Loss: 0.0009545621688317386\n",
      "Total Time Elapsed: 0.671875 seconds\n",
      "Epoch 41/50 Done, Total Loss: 0.0009986943370647362\n",
      "Total Time Elapsed: 0.640625 seconds\n",
      "Epoch 42/50 Done, Total Loss: 0.000818218419912317\n",
      "Total Time Elapsed: 0.671875 seconds\n",
      "Epoch 43/50 Done, Total Loss: 0.0008485725332723289\n",
      "Total Time Elapsed: 0.65625 seconds\n",
      "Epoch 44/50 Done, Total Loss: 0.0008711354448231029\n",
      "Total Time Elapsed: 0.65625 seconds\n",
      "Epoch 45/50 Done, Total Loss: 0.0007909493495568969\n",
      "Total Time Elapsed: 0.65625 seconds\n",
      "Epoch 46/50 Done, Total Loss: 0.0009451589072848381\n",
      "Total Time Elapsed: 0.671875 seconds\n",
      "Epoch 47/50 Done, Total Loss: 0.00070573217769871\n",
      "Total Time Elapsed: 0.65625 seconds\n",
      "Epoch 48/50 Done, Total Loss: 0.0007566636400564467\n",
      "Total Time Elapsed: 0.65625 seconds\n",
      "Epoch 49/50 Done, Total Loss: 0.0006982954214212531\n",
      "Total Time Elapsed: 0.640625 seconds\n",
      "Epoch 50/50 Done, Total Loss: 0.0006879706097457589\n",
      "Total Time Elapsed: 0.671875 seconds\n",
      "Total Training Time: 33.5625 seconds\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "gru_model = train(train_loader,lr,EPOCHS=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Time: 0.421875\n",
      "MAE: 11.453980445861816\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, test_x, test_y, test_units):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    start_time = time.process_time()\n",
    "\n",
    "    for i in range(1,100+1):\n",
    "        ind = np.where(test_units == i)\n",
    "        unit_x = test_x[ind,:]\n",
    "        unit_y = test_y[ind]\n",
    "\n",
    "        inp = torch.from_numpy(unit_x)\n",
    "        labs = torch.from_numpy(unit_y)\n",
    "        h = model.init_hidden(inp.shape[0])\n",
    "        out, h = model(inp.to(device).float(), h)\n",
    "        out = 65*(out+1)\n",
    "        target = 65*(unit_y[-1]+1)\n",
    "\n",
    "        outputs.append(out)\n",
    "        targets.append(target)\n",
    "\n",
    "        \n",
    "    print(\"Evaluation Time: {}\".format(str(time.process_time()-start_time)))\n",
    "    sMAPE = 0\n",
    "    MAE = 0\n",
    "    for i in range(len(outputs)):\n",
    "        MAE += abs(outputs[i] - targets[i])/len(outputs)\n",
    "        # sMAPE += np.mean(abs(outputs[i]-targets[i])/(targets[i]+outputs[i])/2)/len(outputs)\n",
    "    print(f\"MAE: {MAE.item()}\")\n",
    "    return\n",
    "    return outputs, targets, sMAPE\n",
    "\n",
    "evaluate(gru_model,test_x,test_y,test_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
